{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Generator for Dinosaurs\n",
    "\n",
    "### @Author : HADDOU Amine\n",
    "\n",
    "The goal of this project is to developp different neural network models to generates new dinosaur names.<br>\n",
    "The objective is to developp the following two models : \n",
    "- n-grams model language\n",
    "- a pre-trained model (from hugginface) finetuned to the project goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data from `data/dinos.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/dinos.txt\", names=[\"dino_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dino_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aardonyx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abdallahsaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abelisaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abrictosaurus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dino_name\n",
       "0   Aachenosaurus\n",
       "1        Aardonyx\n",
       "2  Abdallahsaurus\n",
       "3     Abelisaurus\n",
       "4   Abrictosaurus"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an overview of the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1536 entries, 0 to 1535\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   dino_name  1536 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 12.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring 20 random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dino_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Rinchenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Amazonsaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>Seitaad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>Wakinosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Rugops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Ojoraptorsaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>LisboasaurusLiubangosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Mosaiceratops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Lanasaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>Diceratops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Craterosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Osmakasaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abrosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>ZatomusZby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>Selimanosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Prenoceratops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Daxiatitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>Xiaosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Antarctopelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aachenosaurus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       dino_name\n",
       "1156                   Rinchenia\n",
       "66                  Amazonsaurus\n",
       "1213                     Seitaad\n",
       "1454                Wakinosaurus\n",
       "1167                      Rugops\n",
       "956              Ojoraptorsaurus\n",
       "791   LisboasaurusLiubangosaurus\n",
       "897                Mosaiceratops\n",
       "744                   Lanasaurus\n",
       "384                   Diceratops\n",
       "337                Craterosaurus\n",
       "982                 Osmakasaurus\n",
       "5                     Abrosaurus\n",
       "1515                  ZatomusZby\n",
       "1214              Selimanosaurus\n",
       "1082               Prenoceratops\n",
       "372                   Daxiatitan\n",
       "1474                  Xiaosaurus\n",
       "96                 Antarctopelta\n",
       "0                  Aachenosaurus"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there punctuation (composed names, etc) ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-alphabetical characters found in the dataset\n"
     ]
    }
   ],
   "source": [
    "def print_non_alphabetical_chars(word):\n",
    "    non_alphabetical = [char for char in word if char.lower() not in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "    if non_alphabetical:\n",
    "        print(\"Non-alphabetical characters in '{}': {}\".format(word, ''.join(non_alphabetical)))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def print_non_alphabetical(data):\n",
    "    presence = False # flag to check if there are any non-alphabetical characters\n",
    "    for name in data[\"dino_name\"]:\n",
    "        if print_non_alphabetical_chars(name):\n",
    "            presence = True\n",
    "    if not presence:\n",
    "        print(\"No non-alphabetical characters found in the dataset\")\n",
    "\n",
    "print_non_alphabetical(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data contains recurrent names ? If so, we should get rid of them later in the pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the dataset: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicates in the dataset: {}\".format(data.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore words length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average length of dino names\n",
    "avg_word_length = data[\"dino_name\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length of dino names\n",
    "max_word_length = data[\"dino_name\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min length of dino names\n",
    "min_word_length = data[\"dino_name\"].str.len().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile of dino names\n",
    "quantile = data[\"dino_name\"].str.len().quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique dino names\n",
    "unique_dino_names = data[\"dino_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 11.962239583333334\n",
      "Max word length: 26\n",
      "Min word length: 3\n",
      "Quantiles : {25% : 10.0, 50% : 12.0, 75% : 13.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average word length: {avg_word_length}\")\n",
    "print(f\"Max word length: {max_word_length}\")\n",
    "print(f\"Min word length: {min_word_length}\")\n",
    "print(f\"Quantiles : {{25% : {quantile[0.25]}, 50% : {quantile[0.5]}, 75% : {quantile[0.75]}}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are interesting. Dinosaur names are, in average, very long. So, if I want to generate a name, model has to take in account enough context from previous letters to generate remaining part.\n",
    "\n",
    "My first idea for the __n_gram model__ is to select `n_gram=3`. Because, I want to catch syllables in name. In my opinion, last syllables is enough context. It corresponds to the last 3 words before the word we want to predict.<br>\n",
    "In this context, a *syllable* is defined as a sequence of three characters, usually at least the middle one is a vowel.<br>\n",
    "It seems that using trigrams (3-grams) is a good choice since many words have lengths that are multiples of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should start with lower case all letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dino_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardonyx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dino_name\n",
       "0  aachenosaurus\n",
       "1       aardonyx"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"dino_name\"] = data[\"dino_name\"].str.lower()\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Recurrent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to delete previous identified dublicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in dino names: 0\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset='dino_name')\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(f\"Number of duplicates in dino names: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add padding at the end of worlds to regularize names' length. We will try the `n-gram` model without padding.<br>\n",
    "Maybe by adding a padding, the model will be able to learn __how__ dino names finish usally (\"aurus\", \"tor\", etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(names: list[str], max_length: int) -> list[str]:\n",
    "    padded_names = []\n",
    "    for name in names :\n",
    "        if len(name) < max_length:\n",
    "            padded_names.append(name + \"1\" * (max_length - len(name)))\n",
    "        else:\n",
    "            padded_names.append(name[:max_length])\n",
    "    return padded_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"paddded_dino_name\"] = add_padding(data[\"dino_name\"].values, max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dino_name</th>\n",
       "      <th>paddded_dino_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachenosaurus</td>\n",
       "      <td>aachenosaurus1111111111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardonyx</td>\n",
       "      <td>aardonyx111111111111111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dino_name           paddded_dino_name\n",
       "0  aachenosaurus  aachenosaurus1111111111111\n",
       "1       aardonyx  aardonyx111111111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start and End Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add `0` at the start and `1` at the end of names. The `0` will be usefull at the begenning of the generation to generate a first letter. For `k-gram`model, we will generate `k` `0` at the begenning to generate the first letter.\n",
    "\n",
    "For the `1`, it will help us determine when the generation is finished. And it may help the model in *learning* how dino names end (\"tor\", \"saurus\",etc).. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(names: list[str], n: int) -> list[str]:\n",
    "    \"\"\"Adds start '0' and end '1' tokens to each name.\"\"\"\n",
    "    return [\"0\" * n + name + \"1\" for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a first function to generates a list of n-grams from a list of names at a **character** level.\n",
    "\n",
    "When building the vocabulary and tokens, we will add an `<UNK>` token that will be used for any unseen tokens or characters during generation. it is important because if the model produce a token which was not in the initial dataset, it won't be able to complete the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_tokens(names: list[str], n: int) -> list[str]:\n",
    "    \"\"\"Generates and returns all unique n-grams from the names.\"\"\"\n",
    "    tokens = [\"<UNK>\"]  # Add the <UNK> token at the beginning\n",
    "    for name in names:\n",
    "        for i in range(len(name) - n + 1):\n",
    "            token = name[i:i+n]\n",
    "            if token not in tokens:\n",
    "                tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'dino', 'inos', 'noso', 'osou', 'sour', 'ouru', 'urus']\n"
     ]
    }
   ],
   "source": [
    "print(list_all_tokens([\"dino\", \"dinosour\", \"dinosourus\"], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some usefull functions usefull for this model __only__. Each models needs his own tool functions. Some times the difference is only a line or a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dertermining tokens of language, it is also necessary to determine our vocabulary. In our case, it is all the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(names: list[str]) -> list[str]:\n",
    "    \"\"\"Builds and returns a list of all unique characters (vocabulary) from the names.\"\"\"\n",
    "    vocab = [\"<UNK>\"]  # Add the <UNK> token at the beginning\n",
    "    for name in names:\n",
    "        for letter in name:\n",
    "            if letter not in vocab:\n",
    "                vocab.append(letter)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute probability of a letter appearing after a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probabilities(probabilities: np.ndarray, tokens: list[str], vocab: list[str]) -> None:\n",
    "    \"\"\"Prints the probabilities of generating each letter for each token.\"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"Token: {token}\")\n",
    "        for j, letter in enumerate(vocab):\n",
    "            prob = probabilities[i][j]\n",
    "            if prob > 0:\n",
    "                print(f\"    {letter}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(names: list[str], tokens: list[str], vocab: list[str], n: int) -> np.ndarray:\n",
    "    \"\"\"Computes the conditional probabilities of each token generating a letter from the vocabulary.\"\"\"\n",
    "    probabilities_array = np.zeros((len(tokens), len(vocab)))\n",
    "\n",
    "    # Count occurrences of letters following each token\n",
    "    for name in names:\n",
    "        for i in range(len(name) - n):\n",
    "            token = name[i:i+n]\n",
    "            next_letter = name[i+n]  # The letter after the token\n",
    "            if next_letter in vocab:\n",
    "                token_index = tokens.index(token) if token in tokens else tokens.index(\"<UNK>\")\n",
    "                letter_index = vocab.index(next_letter)\n",
    "                probabilities_array[token_index][letter_index] += 1\n",
    "            else:\n",
    "                print(f\"Letter {next_letter} not in vocabulary\")\n",
    "                probabilities_array[tokens.index(\"<UNK>\")][vocab.index(\"<UNK>\")] += 1\n",
    "\n",
    "    # Normalize the probabilities by dividing by row sums\n",
    "    row_sums = probabilities_array.sum(axis=1, keepdims=True)\n",
    "    probabilities_array = np.divide(probabilities_array, row_sums, where=row_sums != 0, out=probabilities_array)\n",
    "\n",
    "    # print_probabilities(probabilities_array, tokens, vocab)\n",
    "    \n",
    "    return probabilities_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add some randomness and not generating alwas the same name, we will not retrieve the most predicted letter but one of the first k letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_top_k_letters(probabilities: np.array, token_index: int, vocab: list[str], k: int = 5, verbose: bool = False) -> str:\n",
    "    \"\"\"Chooses one of the top k letters based on probabilities. Only selects letters with probability > 0.\"\"\"\n",
    "    \n",
    "    # Sort the probabilities and get indices sorted by highest probability\n",
    "    sorted_indices = np.argsort(probabilities[token_index])[::-1]  # Sort descending\n",
    "    \n",
    "    # Filter out indices where the probability is > 0\n",
    "    valid_indices = [idx for idx in sorted_indices if probabilities[token_index][idx] > 0]\n",
    "\n",
    "    # Adjust k if there are fewer than k valid options\n",
    "    k = min(k, len(valid_indices))\n",
    "    \n",
    "    if k == 0:\n",
    "        # Handle the case where no valid options with prob > 0 exist, fallback to <UNK> or any default behavior\n",
    "        if verbose:\n",
    "            print(\"No valid letters with probability > 0. Falling back to <UNK>.\")\n",
    "        return \"<UNK>\"  # or any other fallback behavior you'd like\n",
    "\n",
    "    # Select the top k valid indices\n",
    "    best_k_indices = valid_indices[:k]\n",
    "    \n",
    "    # Log top k predicted letters\n",
    "    if verbose:\n",
    "        print(f\"\\nTop {k} predicted letters for token index {token_index}:\")\n",
    "        for idx in best_k_indices:\n",
    "            print(f\"Letter: {vocab[idx]}, Probability: {probabilities[token_index][idx]}\")\n",
    "\n",
    "    # Choose one of the top k at random (or return the best if k=1)\n",
    "    if k > 1:\n",
    "        chosen_index = np.random.choice(best_k_indices)\n",
    "    else:\n",
    "        chosen_index = best_k_indices[-1]\n",
    "    \n",
    "    chosen_letter = vocab[chosen_index]\n",
    "    if verbose:\n",
    "        print(f\"Chosen letter: {chosen_letter}\")\n",
    "    \n",
    "    return chosen_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have all the tools to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_predict_new_name(tokens: list[str], vocab: list[str], probabilities: np.array, n: int, max_inter_count : int = 10, k: int = 5, verbose: bool = False) -> str:\n",
    "    \"\"\"Generates a new name using the n-gram model with <UNK> token handling.\"\"\"\n",
    "    generated_name = \"\"  # Start with an empty string\n",
    "    token = \"0\" * n  # Initial token is the start token\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nStarting name generation:\")\n",
    "    \n",
    "    inter_count = 0\n",
    "    while inter_count < max_inter_count:\n",
    "        # Handle unknown token by using <UNK> token\n",
    "        token_index = tokens.index(token) if token in tokens else tokens.index(\"<UNK>\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nCurrent token: {token}\")\n",
    "            print(f\"Name under construction: {generated_name}\")\n",
    "        \n",
    "        if token == \"0\" * n:  # Start token: choose any letter\n",
    "            next_letter = choose_top_k_letters(probabilities, token_index, vocab, k=len(vocab)-1, verbose=verbose)\n",
    "        else:\n",
    "            # Find the next letter using one of the top k probabilities\n",
    "            next_letter = choose_top_k_letters(probabilities, token_index, vocab, k=k, verbose=verbose)\n",
    "        \n",
    "        if next_letter == \"1\":  # End of name (using '1')\n",
    "            if verbose:\n",
    "                print(f\"End of name reached with letter: 1\\n\")\n",
    "            break\n",
    "        \n",
    "        generated_name += next_letter\n",
    "        token = (token + next_letter)[-n:]  # Shift the token by one letter, keeping it at length n\n",
    "        inter_count += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final generated name: {generated_name}\\n\")\n",
    "    return generated_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_model(names: list[str], n: int = 4, num_predictions: int = 5, max_length_output : int = 10, k: int = 5, verbose: bool = False) -> list[str]:\n",
    "    \"\"\"Trains an n-gram model and generates new names.\"\"\"\n",
    "    # Add start/end tokens to names and build model components\n",
    "    n = n - 1 # n-gram model uses n-1 letters to predict the new one. So I'll adjust n here. \n",
    "    names = add_start_end_tokens(names, n)\n",
    "    tokens = list_all_tokens(names, n)\n",
    "    vocab = build_vocabulary(names)\n",
    "    probabilities = compute_probabilities(names, tokens, vocab, n)\n",
    "    \n",
    "    # Generate new names based on the model\n",
    "    generated_names = []\n",
    "    for _ in range(num_predictions):\n",
    "        name = ngram_predict_new_name(tokens, vocab, probabilities, n, max_inter_count=max_length_output, k=k, verbose=verbose)\n",
    "        generated_names.append(name)\n",
    "    \n",
    "    return generated_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Names with n-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting name generation:\n",
      "\n",
      "Current token: 000\n",
      "Name under construction: \n",
      "\n",
      "Top 26 predicted letters for token index 1:\n",
      "Letter: a, Probability: 0.1089238845144357\n",
      "Letter: s, Probability: 0.09448818897637795\n",
      "Letter: p, Probability: 0.07939632545931759\n",
      "Letter: c, Probability: 0.07086614173228346\n",
      "Letter: t, Probability: 0.06430446194225722\n",
      "Letter: m, Probability: 0.05971128608923885\n",
      "Letter: l, Probability: 0.05380577427821522\n",
      "Letter: d, Probability: 0.0531496062992126\n",
      "Letter: b, Probability: 0.04921259842519685\n",
      "Letter: e, Probability: 0.04265091863517061\n",
      "Letter: h, Probability: 0.04199475065616798\n",
      "Letter: g, Probability: 0.03937007874015748\n",
      "Letter: n, Probability: 0.031496062992125984\n",
      "Letter: o, Probability: 0.02690288713910761\n",
      "Letter: r, Probability: 0.026246719160104987\n",
      "Letter: k, Probability: 0.025590551181102362\n",
      "Letter: j, Probability: 0.01706036745406824\n",
      "Letter: z, Probability: 0.01706036745406824\n",
      "Letter: i, Probability: 0.015748031496062992\n",
      "Letter: y, Probability: 0.015748031496062992\n",
      "Letter: f, Probability: 0.013779527559055118\n",
      "Letter: v, Probability: 0.013123359580052493\n",
      "Letter: w, Probability: 0.01115485564304462\n",
      "Letter: u, Probability: 0.01115485564304462\n",
      "Letter: x, Probability: 0.010498687664041995\n",
      "Letter: q, Probability: 0.006561679790026247\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: 00c\n",
      "Name under construction: c\n",
      "\n",
      "Top 5 predicted letters for token index 896:\n",
      "Letter: h, Probability: 0.26851851851851855\n",
      "Letter: a, Probability: 0.2222222222222222\n",
      "Letter: o, Probability: 0.18518518518518517\n",
      "Letter: r, Probability: 0.12962962962962962\n",
      "Letter: e, Probability: 0.10185185185185185\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: 0co\n",
      "Name under construction: co\n",
      "\n",
      "Top 5 predicted letters for token index 1063:\n",
      "Letter: e, Probability: 0.25\n",
      "Letter: l, Probability: 0.2\n",
      "Letter: m, Probability: 0.2\n",
      "Letter: r, Probability: 0.15\n",
      "Letter: n, Probability: 0.15\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: coe\n",
      "Name under construction: coe\n",
      "\n",
      "Top 1 predicted letters for token index 384:\n",
      "Letter: l, Probability: 1.0\n",
      "Chosen letter: l\n",
      "\n",
      "Current token: oel\n",
      "Name under construction: coel\n",
      "\n",
      "Top 4 predicted letters for token index 385:\n",
      "Letter: u, Probability: 0.5625\n",
      "Letter: o, Probability: 0.1875\n",
      "Letter: i, Probability: 0.1875\n",
      "Letter: e, Probability: 0.0625\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: elo\n",
      "Name under construction: coelo\n",
      "\n",
      "Top 5 predicted letters for token index 62:\n",
      "Letter: s, Probability: 0.3103448275862069\n",
      "Letter: p, Probability: 0.2413793103448276\n",
      "Letter: r, Probability: 0.10344827586206896\n",
      "Letter: c, Probability: 0.10344827586206896\n",
      "Letter: n, Probability: 0.06896551724137931\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: loc\n",
      "Name under construction: coeloc\n",
      "\n",
      "Top 5 predicted letters for token index 321:\n",
      "Letter: e, Probability: 0.2857142857142857\n",
      "Letter: h, Probability: 0.21428571428571427\n",
      "Letter: i, Probability: 0.21428571428571427\n",
      "Letter: o, Probability: 0.14285714285714285\n",
      "Letter: a, Probability: 0.14285714285714285\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: oco\n",
      "Name under construction: coeloco\n",
      "\n",
      "Top 5 predicted letters for token index 322:\n",
      "Letter: e, Probability: 0.5833333333333334\n",
      "Letter: l, Probability: 0.08333333333333333\n",
      "Letter: n, Probability: 0.08333333333333333\n",
      "Letter: d, Probability: 0.08333333333333333\n",
      "Letter: m, Probability: 0.08333333333333333\n",
      "Chosen letter: m\n",
      "\n",
      "Current token: com\n",
      "Name under construction: coelocom\n",
      "\n",
      "Top 3 predicted letters for token index 1078:\n",
      "Letter: p, Probability: 0.5\n",
      "Letter: a, Probability: 0.3333333333333333\n",
      "Letter: i, Probability: 0.16666666666666666\n",
      "Chosen letter: p\n",
      "\n",
      "Current token: omp\n",
      "Name under construction: coelocomp\n",
      "\n",
      "Top 1 predicted letters for token index 1083:\n",
      "Letter: s, Probability: 1.0\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: mps\n",
      "Name under construction: coelocomps\n",
      "\n",
      "Top 1 predicted letters for token index 1084:\n",
      "Letter: o, Probability: 1.0\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: pso\n",
      "Name under construction: coelocompso\n",
      "\n",
      "Top 2 predicted letters for token index 1085:\n",
      "Letter: g, Probability: 0.6666666666666666\n",
      "Letter: s, Probability: 0.3333333333333333\n",
      "Chosen letter: g\n",
      "\n",
      "Current token: sog\n",
      "Name under construction: coelocompsog\n",
      "\n",
      "Top 1 predicted letters for token index 1086:\n",
      "Letter: n, Probability: 1.0\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: ogn\n",
      "Name under construction: coelocompsogn\n",
      "\n",
      "Top 2 predicted letters for token index 747:\n",
      "Letter: a, Probability: 0.8888888888888888\n",
      "Letter: k, Probability: 0.1111111111111111\n",
      "Chosen letter: k\n",
      "\n",
      "Current token: gnk\n",
      "Name under construction: coelocompsognk\n",
      "\n",
      "Top 1 predicted letters for token index 1450:\n",
      "Letter: o, Probability: 1.0\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: nko\n",
      "Name under construction: coelocompsognko\n",
      "\n",
      "Top 3 predicted letters for token index 984:\n",
      "Letter: s, Probability: 0.5\n",
      "Letter: u, Probability: 0.25\n",
      "Letter: n, Probability: 0.25\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: kon\n",
      "Name under construction: coelocompsognkon\n",
      "\n",
      "Top 1 predicted letters for token index 2055:\n",
      "Letter: o, Probability: 1.0\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: ono\n",
      "Name under construction: coelocompsognkono\n",
      "\n",
      "Top 5 predicted letters for token index 895:\n",
      "Letter: s, Probability: 0.47619047619047616\n",
      "Letter: d, Probability: 0.14285714285714285\n",
      "Letter: n, Probability: 0.09523809523809523\n",
      "Letter: t, Probability: 0.09523809523809523\n",
      "Letter: l, Probability: 0.047619047619047616\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: non\n",
      "Name under construction: coelocompsognkonon\n",
      "\n",
      "Top 3 predicted letters for token index 1178:\n",
      "Letter: y, Probability: 0.6666666666666666\n",
      "Letter: e, Probability: 0.16666666666666666\n",
      "Letter: t, Probability: 0.16666666666666666\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: ont\n",
      "Name under construction: coelocompsognkonont\n",
      "\n",
      "Top 2 predicted letters for token index 471:\n",
      "Letter: o, Probability: 0.9\n",
      "Letter: a, Probability: 0.1\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: nta\n",
      "Name under construction: coelocompsognkononta\n",
      "\n",
      "Top 5 predicted letters for token index 457:\n",
      "Letter: s, Probability: 0.26666666666666666\n",
      "Letter: r, Probability: 0.26666666666666666\n",
      "Letter: n, Probability: 0.13333333333333333\n",
      "Letter: i, Probability: 0.06666666666666667\n",
      "Letter: v, Probability: 0.06666666666666667\n",
      "Chosen letter: v\n",
      "\n",
      "Current token: tav\n",
      "Name under construction: coelocompsognkonontav\n",
      "\n",
      "Top 3 predicted letters for token index 91:\n",
      "Letter: e, Probability: 0.6\n",
      "Letter: u, Probability: 0.2\n",
      "Letter: i, Probability: 0.2\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: ave\n",
      "Name under construction: coelocompsognkonontave\n",
      "\n",
      "Top 3 predicted letters for token index 277:\n",
      "Letter: n, Probability: 0.7777777777777778\n",
      "Letter: r, Probability: 0.1111111111111111\n",
      "Letter: i, Probability: 0.1111111111111111\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: vei\n",
      "Name under construction: coelocompsognkonontavei\n",
      "\n",
      "Top 1 predicted letters for token index 2545:\n",
      "Letter: r, Probability: 1.0\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: eir\n",
      "Name under construction: coelocompsognkonontaveir\n",
      "\n",
      "Top 2 predicted letters for token index 633:\n",
      "Letter: o, Probability: 0.5714285714285714\n",
      "Letter: u, Probability: 0.42857142857142855\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: iru\n",
      "Name under construction: coelocompsognkonontaveiru\n",
      "\n",
      "Top 1 predicted letters for token index 634:\n",
      "Letter: s, Probability: 1.0\n",
      "Chosen letter: s\n",
      "Final generated name: coelocompsognkonontaveirus\n",
      "\n",
      "\n",
      "Starting name generation:\n",
      "\n",
      "Current token: 000\n",
      "Name under construction: \n",
      "\n",
      "Top 26 predicted letters for token index 1:\n",
      "Letter: a, Probability: 0.1089238845144357\n",
      "Letter: s, Probability: 0.09448818897637795\n",
      "Letter: p, Probability: 0.07939632545931759\n",
      "Letter: c, Probability: 0.07086614173228346\n",
      "Letter: t, Probability: 0.06430446194225722\n",
      "Letter: m, Probability: 0.05971128608923885\n",
      "Letter: l, Probability: 0.05380577427821522\n",
      "Letter: d, Probability: 0.0531496062992126\n",
      "Letter: b, Probability: 0.04921259842519685\n",
      "Letter: e, Probability: 0.04265091863517061\n",
      "Letter: h, Probability: 0.04199475065616798\n",
      "Letter: g, Probability: 0.03937007874015748\n",
      "Letter: n, Probability: 0.031496062992125984\n",
      "Letter: o, Probability: 0.02690288713910761\n",
      "Letter: r, Probability: 0.026246719160104987\n",
      "Letter: k, Probability: 0.025590551181102362\n",
      "Letter: j, Probability: 0.01706036745406824\n",
      "Letter: z, Probability: 0.01706036745406824\n",
      "Letter: i, Probability: 0.015748031496062992\n",
      "Letter: y, Probability: 0.015748031496062992\n",
      "Letter: f, Probability: 0.013779527559055118\n",
      "Letter: v, Probability: 0.013123359580052493\n",
      "Letter: w, Probability: 0.01115485564304462\n",
      "Letter: u, Probability: 0.01115485564304462\n",
      "Letter: x, Probability: 0.010498687664041995\n",
      "Letter: q, Probability: 0.006561679790026247\n",
      "Chosen letter: y\n",
      "\n",
      "Current token: 00y\n",
      "Name under construction: y\n",
      "\n",
      "Top 5 predicted letters for token index 2720:\n",
      "Letter: u, Probability: 0.4166666666666667\n",
      "Letter: i, Probability: 0.25\n",
      "Letter: a, Probability: 0.20833333333333334\n",
      "Letter: e, Probability: 0.08333333333333333\n",
      "Letter: o, Probability: 0.041666666666666664\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: 0yi\n",
      "Name under construction: yi\n",
      "\n",
      "Top 5 predicted letters for token index 2730:\n",
      "Letter: n, Probability: 0.3333333333333333\n",
      "Letter: m, Probability: 0.16666666666666666\n",
      "Letter: x, Probability: 0.16666666666666666\n",
      "Letter: b, Probability: 0.16666666666666666\n",
      "Letter: z, Probability: 0.16666666666666666\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: yin\n",
      "Name under construction: yin\n",
      "\n",
      "Top 2 predicted letters for token index 2733:\n",
      "Letter: g, Probability: 0.5\n",
      "Letter: l, Probability: 0.5\n",
      "Chosen letter: g\n",
      "\n",
      "Current token: ing\n",
      "Name under construction: ying\n",
      "\n",
      "Top 5 predicted letters for token index 241:\n",
      "Letter: o, Probability: 0.3333333333333333\n",
      "Letter: s, Probability: 0.08333333333333333\n",
      "Letter: e, Probability: 0.08333333333333333\n",
      "Letter: x, Probability: 0.08333333333333333\n",
      "Letter: k, Probability: 0.041666666666666664\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: ngo\n",
      "Name under construction: yingo\n",
      "\n",
      "Top 5 predicted letters for token index 441:\n",
      "Letter: s, Probability: 0.7551020408163265\n",
      "Letter: n, Probability: 0.061224489795918366\n",
      "Letter: p, Probability: 0.061224489795918366\n",
      "Letter: l, Probability: 0.04081632653061224\n",
      "Letter: v, Probability: 0.02040816326530612\n",
      "Chosen letter: l\n",
      "\n",
      "Current token: gol\n",
      "Name under construction: yingol\n",
      "\n",
      "Top 2 predicted letters for token index 442:\n",
      "Letter: a, Probability: 0.5\n",
      "Letter: o, Probability: 0.5\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: olo\n",
      "Name under construction: yingolo\n",
      "\n",
      "Top 5 predicted letters for token index 118:\n",
      "Letter: s, Probability: 0.4166666666666667\n",
      "Letter: p, Probability: 0.3333333333333333\n",
      "Letter: r, Probability: 0.125\n",
      "Letter: n, Probability: 0.08333333333333333\n",
      "Letter: c, Probability: 0.041666666666666664\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: lor\n",
      "Name under construction: yingolor\n",
      "\n",
      "Top 5 predicted letters for token index 777:\n",
      "Letter: o, Probability: 0.3333333333333333\n",
      "Letter: a, Probability: 0.2222222222222222\n",
      "Letter: i, Probability: 0.2222222222222222\n",
      "Letter: h, Probability: 0.1111111111111111\n",
      "Letter: u, Probability: 0.1111111111111111\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: oro\n",
      "Name under construction: yingoloro\n",
      "\n",
      "Top 5 predicted letters for token index 815:\n",
      "Letter: s, Probability: 0.6\n",
      "Letter: p, Probability: 0.06666666666666667\n",
      "Letter: t, Probability: 0.06666666666666667\n",
      "Letter: d, Probability: 0.06666666666666667\n",
      "Letter: g, Probability: 0.06666666666666667\n",
      "Chosen letter: g\n",
      "\n",
      "Current token: rog\n",
      "Name under construction: yingolorog\n",
      "\n",
      "Top 2 predicted letters for token index 816:\n",
      "Letter: r, Probability: 0.5\n",
      "Letter: o, Probability: 0.5\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: ogo\n",
      "Name under construction: yingolorogo\n",
      "\n",
      "Top 3 predicted letters for token index 817:\n",
      "Letter: v, Probability: 0.3333333333333333\n",
      "Letter: n, Probability: 0.3333333333333333\n",
      "Letter: s, Probability: 0.3333333333333333\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: gon\n",
      "Name under construction: yingolorogon\n",
      "\n",
      "Top 5 predicted letters for token index 1382:\n",
      "Letter: g, Probability: 0.46153846153846156\n",
      "Letter: o, Probability: 0.23076923076923078\n",
      "Letter: i, Probability: 0.15384615384615385\n",
      "Letter: d, Probability: 0.07692307692307693\n",
      "Letter: y, Probability: 0.07692307692307693\n",
      "Chosen letter: g\n",
      "\n",
      "Current token: ong\n",
      "Name under construction: yingolorogong\n",
      "\n",
      "Top 5 predicted letters for token index 740:\n",
      "Letter: 1, Probability: 0.4918032786885246\n",
      "Letter: o, Probability: 0.18032786885245902\n",
      "Letter: y, Probability: 0.04918032786885246\n",
      "Letter: b, Probability: 0.04918032786885246\n",
      "Letter: j, Probability: 0.04918032786885246\n",
      "Chosen letter: b\n",
      "\n",
      "Current token: ngb\n",
      "Name under construction: yingolorogongb\n",
      "\n",
      "Top 3 predicted letters for token index 1229:\n",
      "Letter: u, Probability: 0.5\n",
      "Letter: e, Probability: 0.25\n",
      "Letter: a, Probability: 0.25\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: gbe\n",
      "Name under construction: yingolorogongbe\n",
      "\n",
      "Top 1 predicted letters for token index 1230:\n",
      "Letter: i, Probability: 1.0\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: bei\n",
      "Name under construction: yingolorogongbei\n",
      "\n",
      "Top 5 predicted letters for token index 736:\n",
      "Letter: p, Probability: 0.2857142857142857\n",
      "Letter: s, Probability: 0.2857142857142857\n",
      "Letter: t, Probability: 0.14285714285714285\n",
      "Letter: b, Probability: 0.14285714285714285\n",
      "Letter: l, Probability: 0.14285714285714285\n",
      "Chosen letter: p\n",
      "\n",
      "Current token: eip\n",
      "Name under construction: yingolorogongbeip\n",
      "\n",
      "Top 2 predicted letters for token index 742:\n",
      "Letter: i, Probability: 0.6666666666666666\n",
      "Letter: s, Probability: 0.3333333333333333\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: ipi\n",
      "Name under construction: yingolorogongbeipi\n",
      "\n",
      "Top 2 predicted letters for token index 743:\n",
      "Letter: a, Probability: 0.6666666666666666\n",
      "Letter: o, Probability: 0.3333333333333333\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: pia\n",
      "Name under construction: yingolorogongbeipia\n",
      "\n",
      "Top 2 predicted letters for token index 744:\n",
      "Letter: o, Probability: 0.6666666666666666\n",
      "Letter: t, Probability: 0.3333333333333333\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: iat\n",
      "Name under construction: yingolorogongbeipiat\n",
      "\n",
      "Top 5 predicted letters for token index 583:\n",
      "Letter: i, Probability: 0.42857142857142855\n",
      "Letter: o, Probability: 0.14285714285714285\n",
      "Letter: n, Probability: 0.14285714285714285\n",
      "Letter: s, Probability: 0.14285714285714285\n",
      "Letter: y, Probability: 0.14285714285714285\n",
      "Chosen letter: y\n",
      "\n",
      "Current token: aty\n",
      "Name under construction: yingolorogongbeipiaty\n",
      "\n",
      "Top 2 predicted letters for token index 642:\n",
      "Letter: r, Probability: 0.75\n",
      "Letter: c, Probability: 0.25\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: tyr\n",
      "Name under construction: yingolorogongbeipiatyr\n",
      "\n",
      "Top 2 predicted letters for token index 643:\n",
      "Letter: a, Probability: 0.9285714285714286\n",
      "Letter: o, Probability: 0.07142857142857142\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: yro\n",
      "Name under construction: yingolorogongbeipiatyro\n",
      "\n",
      "Top 4 predicted letters for token index 551:\n",
      "Letter: s, Probability: 0.5\n",
      "Letter: p, Probability: 0.25\n",
      "Letter: r, Probability: 0.125\n",
      "Letter: n, Probability: 0.125\n",
      "Chosen letter: p\n",
      "\n",
      "Current token: rop\n",
      "Name under construction: yingolorogongbeipiatyrop\n",
      "\n",
      "Top 5 predicted letters for token index 587:\n",
      "Letter: h, Probability: 0.4\n",
      "Letter: e, Probability: 0.16\n",
      "Letter: a, Probability: 0.16\n",
      "Letter: l, Probability: 0.12\n",
      "Letter: o, Probability: 0.08\n",
      "Chosen letter: h\n",
      "\n",
      "Current token: oph\n",
      "Name under construction: yingolorogongbeipiatyroph\n",
      "\n",
      "Top 5 predicted letters for token index 56:\n",
      "Letter: o, Probability: 0.4\n",
      "Letter: u, Probability: 0.2571428571428571\n",
      "Letter: a, Probability: 0.11428571428571428\n",
      "Letter: y, Probability: 0.11428571428571428\n",
      "Letter: e, Probability: 0.08571428571428572\n",
      "Chosen letter: a\n",
      "Final generated name: yingolorogongbeipiatyropha\n",
      "\n",
      "\n",
      "Starting name generation:\n",
      "\n",
      "Current token: 000\n",
      "Name under construction: \n",
      "\n",
      "Top 26 predicted letters for token index 1:\n",
      "Letter: a, Probability: 0.1089238845144357\n",
      "Letter: s, Probability: 0.09448818897637795\n",
      "Letter: p, Probability: 0.07939632545931759\n",
      "Letter: c, Probability: 0.07086614173228346\n",
      "Letter: t, Probability: 0.06430446194225722\n",
      "Letter: m, Probability: 0.05971128608923885\n",
      "Letter: l, Probability: 0.05380577427821522\n",
      "Letter: d, Probability: 0.0531496062992126\n",
      "Letter: b, Probability: 0.04921259842519685\n",
      "Letter: e, Probability: 0.04265091863517061\n",
      "Letter: h, Probability: 0.04199475065616798\n",
      "Letter: g, Probability: 0.03937007874015748\n",
      "Letter: n, Probability: 0.031496062992125984\n",
      "Letter: o, Probability: 0.02690288713910761\n",
      "Letter: r, Probability: 0.026246719160104987\n",
      "Letter: k, Probability: 0.025590551181102362\n",
      "Letter: j, Probability: 0.01706036745406824\n",
      "Letter: z, Probability: 0.01706036745406824\n",
      "Letter: i, Probability: 0.015748031496062992\n",
      "Letter: y, Probability: 0.015748031496062992\n",
      "Letter: f, Probability: 0.013779527559055118\n",
      "Letter: v, Probability: 0.013123359580052493\n",
      "Letter: w, Probability: 0.01115485564304462\n",
      "Letter: u, Probability: 0.01115485564304462\n",
      "Letter: x, Probability: 0.010498687664041995\n",
      "Letter: q, Probability: 0.006561679790026247\n",
      "Chosen letter: h\n",
      "\n",
      "Current token: 00h\n",
      "Name under construction: h\n",
      "\n",
      "Top 5 predicted letters for token index 1577:\n",
      "Letter: a, Probability: 0.234375\n",
      "Letter: e, Probability: 0.234375\n",
      "Letter: u, Probability: 0.21875\n",
      "Letter: y, Probability: 0.125\n",
      "Letter: o, Probability: 0.109375\n",
      "Chosen letter: y\n",
      "\n",
      "Current token: 0hy\n",
      "Name under construction: hy\n",
      "\n",
      "Top 2 predicted letters for token index 1660:\n",
      "Letter: p, Probability: 0.75\n",
      "Letter: l, Probability: 0.25\n",
      "Chosen letter: l\n",
      "\n",
      "Current token: hyl\n",
      "Name under construction: hyl\n",
      "\n",
      "Top 3 predicted letters for token index 834:\n",
      "Letter: o, Probability: 0.6\n",
      "Letter: a, Probability: 0.2\n",
      "Letter: l, Probability: 0.2\n",
      "Chosen letter: l\n",
      "\n",
      "Current token: yll\n",
      "Name under construction: hyll\n",
      "\n",
      "Top 1 predicted letters for token index 2259:\n",
      "Letter: o, Probability: 1.0\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: llo\n",
      "Name under construction: hyllo\n",
      "\n",
      "Top 5 predicted letters for token index 81:\n",
      "Letter: s, Probability: 0.4166666666666667\n",
      "Letter: d, Probability: 0.25\n",
      "Letter: r, Probability: 0.08333333333333333\n",
      "Letter: p, Probability: 0.08333333333333333\n",
      "Letter: v, Probability: 0.08333333333333333\n",
      "Chosen letter: v\n",
      "\n",
      "Current token: lov\n",
      "Name under construction: hyllov\n",
      "\n",
      "Top 2 predicted letters for token index 630:\n",
      "Letter: e, Probability: 0.6666666666666666\n",
      "Letter: o, Probability: 0.3333333333333333\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: ove\n",
      "Name under construction: hyllove\n",
      "\n",
      "Top 2 predicted letters for token index 168:\n",
      "Letter: n, Probability: 0.9333333333333333\n",
      "Letter: r, Probability: 0.06666666666666667\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: ver\n",
      "Name under construction: hyllover\n",
      "\n",
      "Top 3 predicted letters for token index 790:\n",
      "Letter: s, Probability: 0.5\n",
      "Letter: o, Probability: 0.25\n",
      "Letter: l, Probability: 0.25\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: ers\n",
      "Name under construction: hyllovers\n",
      "\n",
      "Top 2 predicted letters for token index 791:\n",
      "Letter: a, Probability: 0.8\n",
      "Letter: o, Probability: 0.2\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: rso\n",
      "Name under construction: hylloverso\n",
      "\n",
      "Top 2 predicted letters for token index 792:\n",
      "Letter: r, Probability: 0.8333333333333334\n",
      "Letter: s, Probability: 0.16666666666666666\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: sor\n",
      "Name under construction: hylloversor\n",
      "\n",
      "Top 4 predicted letters for token index 793:\n",
      "Letter: 1, Probability: 0.4444444444444444\n",
      "Letter: n, Probability: 0.2222222222222222\n",
      "Letter: i, Probability: 0.2222222222222222\n",
      "Letter: k, Probability: 0.1111111111111111\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: orn\n",
      "Name under construction: hylloversorn\n",
      "\n",
      "Top 2 predicted letters for token index 143:\n",
      "Letter: i, Probability: 0.9354838709677419\n",
      "Letter: a, Probability: 0.06451612903225806\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: rni\n",
      "Name under construction: hylloversorni\n",
      "\n",
      "Top 3 predicted letters for token index 144:\n",
      "Letter: t, Probability: 0.5862068965517241\n",
      "Letter: s, Probability: 0.3793103448275862\n",
      "Letter: e, Probability: 0.034482758620689655\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: nit\n",
      "Name under construction: hylloversornit\n",
      "\n",
      "Top 5 predicted letters for token index 145:\n",
      "Letter: h, Probability: 0.8095238095238095\n",
      "Letter: r, Probability: 0.047619047619047616\n",
      "Letter: a, Probability: 0.047619047619047616\n",
      "Letter: y, Probability: 0.047619047619047616\n",
      "Letter: z, Probability: 0.047619047619047616\n",
      "Chosen letter: y\n",
      "\n",
      "Current token: ity\n",
      "Name under construction: hylloversornity\n",
      "\n",
      "Top 2 predicted letters for token index 194:\n",
      "Letter: s, Probability: 0.5\n",
      "Letter: r, Probability: 0.5\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: tys\n",
      "Name under construction: hylloversornitys\n",
      "\n",
      "Top 2 predicted letters for token index 195:\n",
      "Letter: 1, Probability: 0.5\n",
      "Letter: a, Probability: 0.5\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: ysa\n",
      "Name under construction: hylloversornitysa\n",
      "\n",
      "Top 2 predicted letters for token index 545:\n",
      "Letter: u, Probability: 0.9333333333333333\n",
      "Letter: l, Probability: 0.06666666666666667\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: sau\n",
      "Name under construction: hylloversornitysau\n",
      "\n",
      "Top 1 predicted letters for token index 11:\n",
      "Letter: r, Probability: 1.0\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: aur\n",
      "Name under construction: hylloversornitysaur\n",
      "\n",
      "Top 5 predicted letters for token index 12:\n",
      "Letter: u, Probability: 0.9431216931216931\n",
      "Letter: o, Probability: 0.026455026455026454\n",
      "Letter: a, Probability: 0.022486772486772486\n",
      "Letter: i, Probability: 0.006613756613756613\n",
      "Letter: 1, Probability: 0.0013227513227513227\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: uru\n",
      "Name under construction: hylloversornitysauru\n",
      "\n",
      "Top 4 predicted letters for token index 13:\n",
      "Letter: s, Probability: 0.9958448753462604\n",
      "Letter: t, Probability: 0.0013850415512465374\n",
      "Letter: m, Probability: 0.0013850415512465374\n",
      "Letter: 1, Probability: 0.0013850415512465374\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: rut\n",
      "Name under construction: hylloversornitysaurut\n",
      "\n",
      "Top 2 predicted letters for token index 719:\n",
      "Letter: h, Probability: 0.6666666666666666\n",
      "Letter: i, Probability: 0.3333333333333333\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: uti\n",
      "Name under construction: hylloversornitysauruti\n",
      "\n",
      "Top 3 predicted letters for token index 720:\n",
      "Letter: t, Probability: 0.6\n",
      "Letter: c, Probability: 0.2\n",
      "Letter: s, Probability: 0.2\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: tis\n",
      "Name under construction: hylloversornitysaurutis\n",
      "\n",
      "Top 2 predicted letters for token index 112:\n",
      "Letter: a, Probability: 0.8333333333333334\n",
      "Letter: p, Probability: 0.16666666666666666\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: isa\n",
      "Name under construction: hylloversornitysaurutisa\n",
      "\n",
      "Top 3 predicted letters for token index 36:\n",
      "Letter: u, Probability: 0.9473684210526315\n",
      "Letter: n, Probability: 0.039473684210526314\n",
      "Letter: b, Probability: 0.013157894736842105\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: sau\n",
      "Name under construction: hylloversornitysaurutisau\n",
      "\n",
      "Top 1 predicted letters for token index 11:\n",
      "Letter: r, Probability: 1.0\n",
      "Chosen letter: r\n",
      "Final generated name: hylloversornitysaurutisaur\n",
      "\n",
      "\n",
      "Starting name generation:\n",
      "\n",
      "Current token: 000\n",
      "Name under construction: \n",
      "\n",
      "Top 26 predicted letters for token index 1:\n",
      "Letter: a, Probability: 0.1089238845144357\n",
      "Letter: s, Probability: 0.09448818897637795\n",
      "Letter: p, Probability: 0.07939632545931759\n",
      "Letter: c, Probability: 0.07086614173228346\n",
      "Letter: t, Probability: 0.06430446194225722\n",
      "Letter: m, Probability: 0.05971128608923885\n",
      "Letter: l, Probability: 0.05380577427821522\n",
      "Letter: d, Probability: 0.0531496062992126\n",
      "Letter: b, Probability: 0.04921259842519685\n",
      "Letter: e, Probability: 0.04265091863517061\n",
      "Letter: h, Probability: 0.04199475065616798\n",
      "Letter: g, Probability: 0.03937007874015748\n",
      "Letter: n, Probability: 0.031496062992125984\n",
      "Letter: o, Probability: 0.02690288713910761\n",
      "Letter: r, Probability: 0.026246719160104987\n",
      "Letter: k, Probability: 0.025590551181102362\n",
      "Letter: j, Probability: 0.01706036745406824\n",
      "Letter: z, Probability: 0.01706036745406824\n",
      "Letter: i, Probability: 0.015748031496062992\n",
      "Letter: y, Probability: 0.015748031496062992\n",
      "Letter: f, Probability: 0.013779527559055118\n",
      "Letter: v, Probability: 0.013123359580052493\n",
      "Letter: w, Probability: 0.01115485564304462\n",
      "Letter: u, Probability: 0.01115485564304462\n",
      "Letter: x, Probability: 0.010498687664041995\n",
      "Letter: q, Probability: 0.006561679790026247\n",
      "Chosen letter: d\n",
      "\n",
      "Current token: 00d\n",
      "Name under construction: d\n",
      "\n",
      "Top 5 predicted letters for token index 1136:\n",
      "Letter: a, Probability: 0.2839506172839506\n",
      "Letter: r, Probability: 0.19753086419753085\n",
      "Letter: i, Probability: 0.19753086419753085\n",
      "Letter: e, Probability: 0.09876543209876543\n",
      "Letter: o, Probability: 0.09876543209876543\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: 0di\n",
      "Name under construction: di\n",
      "\n",
      "Top 5 predicted letters for token index 1191:\n",
      "Letter: c, Probability: 0.1875\n",
      "Letter: a, Probability: 0.1875\n",
      "Letter: n, Probability: 0.1875\n",
      "Letter: p, Probability: 0.125\n",
      "Letter: l, Probability: 0.125\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: dic\n",
      "Name under construction: dic\n",
      "\n",
      "Top 3 predicted letters for token index 1195:\n",
      "Letter: e, Probability: 0.6666666666666666\n",
      "Letter: r, Probability: 0.16666666666666666\n",
      "Letter: l, Probability: 0.16666666666666666\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: icr\n",
      "Name under construction: dicr\n",
      "\n",
      "Top 2 predicted letters for token index 1200:\n",
      "Letter: o, Probability: 0.9090909090909091\n",
      "Letter: a, Probability: 0.09090909090909091\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: cra\n",
      "Name under construction: dicra\n",
      "\n",
      "Top 4 predicted letters for token index 1099:\n",
      "Letter: t, Probability: 0.5\n",
      "Letter: n, Probability: 0.16666666666666666\n",
      "Letter: e, Probability: 0.16666666666666666\n",
      "Letter: s, Probability: 0.16666666666666666\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: rat\n",
      "Name under construction: dicrat\n",
      "\n",
      "Top 5 predicted letters for token index 206:\n",
      "Letter: o, Probability: 0.8902439024390244\n",
      "Letter: u, Probability: 0.024390243902439025\n",
      "Letter: e, Probability: 0.024390243902439025\n",
      "Letter: i, Probability: 0.024390243902439025\n",
      "Letter: y, Probability: 0.012195121951219513\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: ati\n",
      "Name under construction: dicrati\n",
      "\n",
      "Top 5 predicted letters for token index 366:\n",
      "Letter: t, Probability: 0.6666666666666666\n",
      "Letter: 1, Probability: 0.09523809523809523\n",
      "Letter: v, Probability: 0.047619047619047616\n",
      "Letter: r, Probability: 0.047619047619047616\n",
      "Letter: s, Probability: 0.047619047619047616\n",
      "Chosen letter: 1\n",
      "End of name reached with letter: 1\n",
      "\n",
      "Final generated name: dicrati\n",
      "\n",
      "\n",
      "Starting name generation:\n",
      "\n",
      "Current token: 000\n",
      "Name under construction: \n",
      "\n",
      "Top 26 predicted letters for token index 1:\n",
      "Letter: a, Probability: 0.1089238845144357\n",
      "Letter: s, Probability: 0.09448818897637795\n",
      "Letter: p, Probability: 0.07939632545931759\n",
      "Letter: c, Probability: 0.07086614173228346\n",
      "Letter: t, Probability: 0.06430446194225722\n",
      "Letter: m, Probability: 0.05971128608923885\n",
      "Letter: l, Probability: 0.05380577427821522\n",
      "Letter: d, Probability: 0.0531496062992126\n",
      "Letter: b, Probability: 0.04921259842519685\n",
      "Letter: e, Probability: 0.04265091863517061\n",
      "Letter: h, Probability: 0.04199475065616798\n",
      "Letter: g, Probability: 0.03937007874015748\n",
      "Letter: n, Probability: 0.031496062992125984\n",
      "Letter: o, Probability: 0.02690288713910761\n",
      "Letter: r, Probability: 0.026246719160104987\n",
      "Letter: k, Probability: 0.025590551181102362\n",
      "Letter: j, Probability: 0.01706036745406824\n",
      "Letter: z, Probability: 0.01706036745406824\n",
      "Letter: i, Probability: 0.015748031496062992\n",
      "Letter: y, Probability: 0.015748031496062992\n",
      "Letter: f, Probability: 0.013779527559055118\n",
      "Letter: v, Probability: 0.013123359580052493\n",
      "Letter: w, Probability: 0.01115485564304462\n",
      "Letter: u, Probability: 0.01115485564304462\n",
      "Letter: x, Probability: 0.010498687664041995\n",
      "Letter: q, Probability: 0.006561679790026247\n",
      "Chosen letter: p\n",
      "\n",
      "Current token: 00p\n",
      "Name under construction: p\n",
      "\n",
      "Top 5 predicted letters for token index 2206:\n",
      "Letter: a, Probability: 0.33884297520661155\n",
      "Letter: r, Probability: 0.2396694214876033\n",
      "Letter: e, Probability: 0.10743801652892562\n",
      "Letter: o, Probability: 0.08264462809917356\n",
      "Letter: l, Probability: 0.05785123966942149\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: 0po\n",
      "Name under construction: po\n",
      "\n",
      "Top 5 predicted letters for token index 2287:\n",
      "Letter: l, Probability: 0.4\n",
      "Letter: p, Probability: 0.1\n",
      "Letter: w, Probability: 0.1\n",
      "Letter: s, Probability: 0.1\n",
      "Letter: d, Probability: 0.1\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: pos\n",
      "Name under construction: pos\n",
      "\n",
      "Top 3 predicted letters for token index 128:\n",
      "Letter: a, Probability: 0.5454545454545454\n",
      "Letter: e, Probability: 0.36363636363636365\n",
      "Letter: t, Probability: 0.09090909090909091\n",
      "Chosen letter: t\n",
      "\n",
      "Current token: ost\n",
      "Name under construction: post\n",
      "\n",
      "Top 4 predicted letters for token index 153:\n",
      "Letter: e, Probability: 0.5714285714285714\n",
      "Letter: r, Probability: 0.2857142857142857\n",
      "Letter: o, Probability: 0.07142857142857142\n",
      "Letter: a, Probability: 0.07142857142857142\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: ste\n",
      "Name under construction: poste\n",
      "\n",
      "Top 5 predicted letters for token index 154:\n",
      "Letter: s, Probability: 0.25806451612903225\n",
      "Letter: g, Probability: 0.1935483870967742\n",
      "Letter: r, Probability: 0.16129032258064516\n",
      "Letter: n, Probability: 0.16129032258064516\n",
      "Letter: u, Probability: 0.0967741935483871\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: teu\n",
      "Name under construction: posteu\n",
      "\n",
      "Top 1 predicted letters for token index 2293:\n",
      "Letter: s, Probability: 1.0\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: eus\n",
      "Name under construction: posteus\n",
      "\n",
      "Top 3 predicted letters for token index 276:\n",
      "Letter: 1, Probability: 0.8947368421052632\n",
      "Letter: t, Probability: 0.05263157894736842\n",
      "Letter: k, Probability: 0.05263157894736842\n",
      "Chosen letter: k\n",
      "\n",
      "Current token: usk\n",
      "Name under construction: posteusk\n",
      "\n",
      "Top 2 predicted letters for token index 1396:\n",
      "Letter: e, Probability: 0.5\n",
      "Letter: o, Probability: 0.5\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: ske\n",
      "Name under construction: posteuske\n",
      "\n",
      "Top 1 predicted letters for token index 1397:\n",
      "Letter: l, Probability: 1.0\n",
      "Chosen letter: l\n",
      "\n",
      "Current token: kel\n",
      "Name under construction: posteuskel\n",
      "\n",
      "Top 4 predicted letters for token index 1148:\n",
      "Letter: e, Probability: 0.4\n",
      "Letter: y, Probability: 0.2\n",
      "Letter: o, Probability: 0.2\n",
      "Letter: m, Probability: 0.2\n",
      "Chosen letter: o\n",
      "\n",
      "Current token: elo\n",
      "Name under construction: posteuskelo\n",
      "\n",
      "Top 5 predicted letters for token index 62:\n",
      "Letter: s, Probability: 0.3103448275862069\n",
      "Letter: p, Probability: 0.2413793103448276\n",
      "Letter: r, Probability: 0.10344827586206896\n",
      "Letter: c, Probability: 0.10344827586206896\n",
      "Letter: n, Probability: 0.06896551724137931\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: los\n",
      "Name under construction: posteuskelos\n",
      "\n",
      "Top 5 predicted letters for token index 136:\n",
      "Letter: a, Probability: 0.8222222222222222\n",
      "Letter: s, Probability: 0.06666666666666667\n",
      "Letter: p, Probability: 0.022222222222222223\n",
      "Letter: i, Probability: 0.022222222222222223\n",
      "Letter: t, Probability: 0.022222222222222223\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: osa\n",
      "Name under construction: posteuskelosa\n",
      "\n",
      "Top 3 predicted letters for token index 10:\n",
      "Letter: u, Probability: 0.9954441913439636\n",
      "Letter: s, Probability: 0.002277904328018223\n",
      "Letter: i, Probability: 0.002277904328018223\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: sai\n",
      "Name under construction: posteuskelosai\n",
      "\n",
      "Top 1 predicted letters for token index 2058:\n",
      "Letter: c, Probability: 1.0\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: aic\n",
      "Name under construction: posteuskelosaic\n",
      "\n",
      "Top 2 predicted letters for token index 2059:\n",
      "Letter: e, Probability: 0.5\n",
      "Letter: h, Probability: 0.5\n",
      "Chosen letter: e\n",
      "\n",
      "Current token: ice\n",
      "Name under construction: posteuskelosaice\n",
      "\n",
      "Top 3 predicted letters for token index 419:\n",
      "Letter: r, Probability: 0.8823529411764706\n",
      "Letter: n, Probability: 0.058823529411764705\n",
      "Letter: i, Probability: 0.058823529411764705\n",
      "Chosen letter: r\n",
      "\n",
      "Current token: cer\n",
      "Name under construction: posteuskelosaicer\n",
      "\n",
      "Top 5 predicted letters for token index 204:\n",
      "Letter: a, Probability: 0.925\n",
      "Letter: o, Probability: 0.0375\n",
      "Letter: n, Probability: 0.0125\n",
      "Letter: c, Probability: 0.0125\n",
      "Letter: v, Probability: 0.0125\n",
      "Chosen letter: a\n",
      "\n",
      "Current token: era\n",
      "Name under construction: posteuskelosaicera\n",
      "\n",
      "Top 5 predicted letters for token index 205:\n",
      "Letter: t, Probability: 0.891566265060241\n",
      "Letter: s, Probability: 0.060240963855421686\n",
      "Letter: p, Probability: 0.024096385542168676\n",
      "Letter: b, Probability: 0.012048192771084338\n",
      "Letter: n, Probability: 0.012048192771084338\n",
      "Chosen letter: s\n",
      "\n",
      "Current token: ras\n",
      "Name under construction: posteuskelosaiceras\n",
      "\n",
      "Top 5 predicted letters for token index 427:\n",
      "Letter: a, Probability: 0.6666666666666666\n",
      "Letter: i, Probability: 0.125\n",
      "Letter: p, Probability: 0.041666666666666664\n",
      "Letter: u, Probability: 0.041666666666666664\n",
      "Letter: 1, Probability: 0.041666666666666664\n",
      "Chosen letter: u\n",
      "\n",
      "Current token: asu\n",
      "Name under construction: posteuskelosaicerasu\n",
      "\n",
      "Top 4 predicted letters for token index 710:\n",
      "Letter: c, Probability: 0.4444444444444444\n",
      "Letter: s, Probability: 0.2222222222222222\n",
      "Letter: t, Probability: 0.2222222222222222\n",
      "Letter: 1, Probability: 0.1111111111111111\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: suc\n",
      "Name under construction: posteuskelosaicerasuc\n",
      "\n",
      "Top 2 predicted letters for token index 555:\n",
      "Letter: h, Probability: 0.9615384615384616\n",
      "Letter: c, Probability: 0.038461538461538464\n",
      "Chosen letter: c\n",
      "\n",
      "Current token: ucc\n",
      "Name under construction: posteuskelosaicerasucc\n",
      "\n",
      "Top 1 predicted letters for token index 2498:\n",
      "Letter: i, Probability: 1.0\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: cci\n",
      "Name under construction: posteuskelosaicerasucci\n",
      "\n",
      "Top 1 predicted letters for token index 2499:\n",
      "Letter: n, Probability: 1.0\n",
      "Chosen letter: n\n",
      "\n",
      "Current token: cin\n",
      "Name under construction: posteuskelosaicerasuccin\n",
      "\n",
      "Top 4 predicted letters for token index 240:\n",
      "Letter: g, Probability: 0.25\n",
      "Letter: i, Probability: 0.25\n",
      "Letter: c, Probability: 0.25\n",
      "Letter: o, Probability: 0.25\n",
      "Chosen letter: i\n",
      "\n",
      "Current token: ini\n",
      "Name under construction: posteuskelosaicerasuccini\n",
      "\n",
      "Top 5 predicted letters for token index 214:\n",
      "Letter: s, Probability: 0.375\n",
      "Letter: o, Probability: 0.125\n",
      "Letter: n, Probability: 0.125\n",
      "Letter: r, Probability: 0.125\n",
      "Letter: z, Probability: 0.125\n",
      "Chosen letter: r\n",
      "Final generated name: posteuskelosaicerasuccinir\n",
      "\n",
      "['coelocompsognkonontaveirus', 'yingolorogongbeipiatyropha', 'hylloversornitysaurutisaur', 'dicrati', 'posteuskelosaicerasuccinir']\n"
     ]
    }
   ],
   "source": [
    "# Generate 5 names with n=4, top 5 letters considered, and verbose logging enabled\n",
    "names = data[\"dino_name\"].values\n",
    "generated_names = ngram_model(names, n=4, max_length_output=max_word_length, num_predictions=5, k=5, verbose=True)\n",
    "\n",
    "print(generated_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of my generator with different parameters' values : \n",
    "-  `n = 3` and `k = 2`<br>\n",
    "['neoversosuccingshadros', 'bator', 'quilmayisaurutichodosuccin', 'dasylossus', 'inosphagros']\n",
    "-  `n = 3` and `k = 3`<br>\n",
    "['yungonius', 'elaplossuesiohadromaia', 'venescelusothostospinax', 'xingsauros', 'euskelyx']\n",
    "-  `n = 2` and `k = 3`<br>\n",
    "['heisaudasilis', 'kan', 'ale', 'xiasaustesaudiangobistrops', 'raptastriong']\n",
    "- `n = 4` and `k=3`<br>\n",
    "['unicerosaurophale', 'yurgovuchia', 'ischyrophus', 'ovirapterovenatosaurus', 'magnapartenykus']\n",
    "- `n = 4` and `k=5`<br>\n",
    "['cristatus', 'vouivria', 'yongjianosaurutitanius', 'mojoceratusauravusaurornit', 'epachthosuchomimoides']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like with a 4-gram model, we generate name that looks like real dino names. Even with a high randomness (k=5), results are still good.\n",
    "\n",
    "Even with the 3-gram model, the results are good (with low randomness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **n-gram model** is a type of probabilistic model used for generating sequences (in this case, names) by predicting the next element in a sequence based on the previous **n-1** elements. The model learns the probability distribution of letters following specific **n-grams** (substrings of length `n`) from a training dataset.\n",
    "\n",
    "This implementation generates new names one letter at a time using the learned probability distribution from the training data. The generated name is built step by step by predicting the next letter based on the preceding **n-1** letters.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components and Parameters\n",
    "\n",
    "#### 1. **`n` (N-gram size)**\n",
    "- Defines the length of the n-grams. \n",
    "- **Example**: If `n=3`, the model will use 2 previous letters to predict the next one (trigram).\n",
    "\n",
    "#### 2. **`list_all_tokens(names, n)`**\n",
    "- **Description**: Generates all unique n-grams from the names and adds a special `<UNK>` token for unknown or unseen tokens.\n",
    "\n",
    "#### 3. **`build_vocabulary(names)`**\n",
    "- **Description**: Builds a vocabulary of all unique letters (characters) in the training data, including the `<UNK>` token.\n",
    "\n",
    "#### 4. **`compute_probabilities(names, tokens, vocab, n)`**\n",
    "- **Description**: Computes the conditional probabilities of generating a letter from the vocabulary based on the preceding n-gram (token). These probabilities are used during name generation.\n",
    "- **Impact**: Determines how likely certain letters are to follow particular n-grams. This affects the diversity and realism of generated names.\n",
    "\n",
    "#### 5. **`choose_top_k_letters(probabilities, token_index, vocab, k)`**\n",
    "- **Description**: Selects the next letter from the top `k` most probable letters based on the learned probabilities.\n",
    "- **Parameters**:\n",
    "  - **`k`**: The number of top letters to consider when selecting the next letter. If `k > 1`, one letter is chosen randomly from the top `k`.\n",
    "  - When selecting the best `k`, if a selected letter has a probability of 0. It is __excluded__ and `k`is automatically decremented for this letter prediction.\n",
    "  - **Impact**: \n",
    "    - Higher `k` introduces more randomness (more variation).\n",
    "    - Lower `k` makes the model pick the highest probability letter, leading to more deterministic behavior.\n",
    "\n",
    "#### 6. **`ngram_predict_new_name(tokens, vocab, probabilities, n, max_inter_count, k)`**\n",
    "- **Description**: Generates a new name by predicting one letter at a time using the n-gram model.\n",
    "- **Parameters**:\n",
    "  - **`max_inter_count`**: The maximum length of the generated name (or maximum number of prediction steps).\n",
    "  - **`k`**: How many of the top predicted letters are considered for the next letter.\n",
    "\n",
    "#### 7. **`ngram_model(names, n, num_predictions, max_length_output, k, verbose)`**\n",
    "- **Description**: Trains the n-gram model and generates `num_predictions` new names.\n",
    "- **Parameters**:\n",
    "  - **`num_predictions`**: Number of new names to generate.\n",
    "  - **`max_length_output`**: Maximum number of characters in each generated name.\n",
    "  - **`verbose`**: Whether to print detailed logs of the name generation process.\n",
    "\n",
    "---\n",
    "\n",
    "### How the Model Works:\n",
    "1. **Training Phase**:\n",
    "   - The model processes the list of names to build the vocabulary and n-gram tokens.\n",
    "   - It calculates the probability of each letter occurring after each token based on the training data.\n",
    "   \n",
    "2. **Generation Phase**:\n",
    "   - The model generates new names by starting with a special start token (e.g., `\"000\"` for `n=4`).\n",
    "   - For each step, it uses the previous n-1 letters to predict the next letter.\n",
    "   - The next letter is chosen based on the learned probabilities, and the process continues until an end token (`\"1\"`) is generated or the name reaches the maximum length (`max_length_output`).\n",
    "\n",
    "---\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "# Example test data (dinosaur names)\n",
    "dino_names = [\"Tyrannosaurus\", \"Stegosaurus\", \"Triceratops\"]\n",
    "\n",
    "# Generate 5 names with n=4, maximum name length of 10, and top 3 letters considered\n",
    "generated_names = ngram_model(dino_names, n=4, num_predictions=5, max_length_output=10, k=3, verbose=True)\n",
    "\n",
    "print(generated_names)\n",
    "```\n",
    "\n",
    "This example generates 5 names with a trigram model, considers the top 3 letters at each step, and limits the names to 10 characters max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) LSTM model - *One letter based*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM It is a type of RNN model usuall used for sequence prediction tasks. It is capable of learning long-term dependencies in data. This is the reason why it is used for text generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we have to firstly define the vocabulary of our language again. It will be different as before because I decided here to define each char as an element of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_classique_LSTM(names : list[str]) :\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "    vocab = list(dict.fromkeys(chars)) # make sure there are no duplicates\n",
    "    char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
    "    index_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "    vocab_size = len(vocab)\n",
    "    return vocab, char_to_index, index_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_to_seq(seq: list[int], max_length: int) -> list[int]:\n",
    "    return [0] * (max_length - len(seq)) + seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this definition, we may encode the data. Otherwise, the model won't be able to learn from dino names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(names: list[str], char_to_index: dict, sequence_length: int = 15) -> tuple[np.array, np.array]: \n",
    "    data_X, data_y = [], []\n",
    "    for name in names:\n",
    "        encoded_name = [char_to_index[char] for char in name]\n",
    "        for i in range(1, len(encoded_name)):\n",
    "            seq_in = encoded_name[:i]\n",
    "            seq_out = encoded_name[i]\n",
    "            seq_in = pad_sequences([seq_in], maxlen=sequence_length, padding='pre')[0]\n",
    "            data_X.append(seq_in)\n",
    "            data_y.append(seq_out)\n",
    "    return np.array(data_X), np.array(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that use previous functions to make last adjustement on dino names (last processing) and then returnin training data, vocabulary and other important informations for the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data: pd.DataFrame, max_length: int) -> tuple[np.array, np.array, list[str], dict, dict, int]:\n",
    "    names = data[\"dino_name\"].values\n",
    "    names = add_start_end_tokens(names, 1)\n",
    "    vocab, char_to_index, index_to_char, vocab_size = build_vocabulary_classique_LSTM(names)\n",
    "    names = add_padding(names, max_length=max_word_length)\n",
    "    data_X, data_y = encoding(names, char_to_index, sequence_length=max_word_length)\n",
    "    return data_X, data_y, vocab, char_to_index, index_to_char, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we are using to generate dinosaur names is based on an LSTM neural network. Here is the configuration of the layers:\n",
    "\n",
    "- **Input Layer**: Accepts input sequences of length `sequence_length`.\n",
    "- **Embedding Layer**: Converts each character (represented by an integer index) into an 13-dimensional vector, helping the model learn relationships between characters. The embedding has `input_dim=vocab_size`, `output_dim=13`, and `input_length=sequence_length`.\n",
    "- **LSTM Layer**: Contains 128 units, allowing the model to learn the temporal dependencies between characters in the sequence, crucial for generating plausible names.\n",
    "- **Dense Layer**: Outputs a probability distribution over the `vocab_size` possible characters using the `softmax` activation function. This allows the model to predict the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(vocab_size: int, sequence_length: int, embedding_size: int = 13, hidden_units: int = 128, show_summary: bool = False) -> Model:\n",
    "    inputs = Input(shape=(sequence_length,))\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size)(inputs)\n",
    "    lstm = LSTM(hidden_units)(embedding)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lstm)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to train. It will be used later for other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model: Model, data_X: np.array, data_y: np.array, epochs: int = 100, batch_size: int = 32) -> Model:\n",
    "    # Train the model\n",
    "    mycallback = EarlyStopping(monitor='loss', patience=5)\n",
    "    model.fit(data_X, data_y, epochs=epochs, batch_size=batch_size, callbacks=[mycallback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly, let's excute the preporcess on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X, data_y, vocab, char_to_index, index_to_char, vocab_size = prepare_training_data(data, max_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initiallisation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 1.4972\n",
      "Epoch 2/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - loss: 1.0672\n",
      "Epoch 3/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - loss: 0.9797\n",
      "Epoch 4/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 0.9370\n",
      "Epoch 5/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 0.8927\n",
      "Epoch 6/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 0.8480\n",
      "Epoch 7/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - loss: 0.8329\n",
      "Epoch 8/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - loss: 0.7992\n",
      "Epoch 9/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - loss: 0.7809\n",
      "Epoch 10/10\n",
      "\u001b[1m1191/1191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - loss: 0.7639\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_LSTM_model(vocab_size, max_word_length)\n",
    "lstm_model = training(lstm_model, data_X, data_y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model under lstm name\n",
    "lstm_model.save(\"../models/lstm_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for generating new names using the new LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a new name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal of the following function is to introduce some kind of randomness in the generator. Not alway predicting the most probable letter but one the k-most probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_top_k_letters_from_model(predictions: np.array, k: int = 5) -> int:\n",
    "    \"\"\"Chooses one of the top k letters based on probabilities. Only selects letters with probability > 0.\"\"\"\n",
    "    sorted_indices = np.argsort(predictions)[::-1]  # Sort descending\n",
    "    # Filter out indices where the probability is > 0\n",
    "    valid_indices = [idx for idx in sorted_indices if predictions[idx] > 0]\n",
    "    # Adjust k if there are fewer than k valid options\n",
    "    k = min(k, len(valid_indices))\n",
    "    if k == 0:\n",
    "        print(\"No valid letters with probability > 0.\")\n",
    "        # Return random letter if no valid letter found (except 0)\n",
    "        letter = 0\n",
    "        while letter == 0:\n",
    "            letter = np.random.choice(len(predictions))\n",
    "        return letter\n",
    "\n",
    "    # Select the top k valid indices\n",
    "    best_k_indices = valid_indices[:k]\n",
    "    \n",
    "    # Choose one of the top k at random (or return the best if k=1)\n",
    "    if k > 1:\n",
    "        chosen_index = np.random.choice(best_k_indices)\n",
    "    else:\n",
    "        chosen_index = best_k_indices[-1]\n",
    "    \n",
    "    return chosen_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function to generate one new name only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model: \"Model\", char_to_index: dict, index_to_char: dict, sequence_length: int, max_length: int = 20, k: int = 5) -> str:\n",
    "    name = [0]  # start token ('0')\n",
    "    while len(name) < max_length:\n",
    "        seq_in = add_padding_to_seq(name, sequence_length)\n",
    "        prediction = model.predict(np.array([seq_in]), verbose=0)\n",
    "        next_index = choose_top_k_letters_from_model(prediction[0], k)\n",
    "        if next_index == 1:  # end token ('1')\n",
    "            break\n",
    "        name.append(next_index)\n",
    "    return ''.join([index_to_char[idx] for idx in name[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to generates multiple names at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_names(model: \"Model\", char_to_index: dict, index_to_char: dict, sequence_length: int, n: int, k: int = 5) -> list[str]:\n",
    "    names = []\n",
    "    for _ in range(n):\n",
    "        name = generate_name(model, char_to_index=char_to_index, index_to_char=index_to_char, sequence_length=sequence_length, k=k)\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k`(temperature) = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calionn', 'alacetialususucandu', 'arconykasuss', 'silophysis', 'caleospenuslulumurn']\n"
     ]
    }
   ],
   "source": [
    "names = generate_n_names(lstm_model, char_to_index=char_to_index, index_to_char=index_to_char, sequence_length=max_word_length, n=5, k=3)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k`= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sauarodosaidudosa', 'siniasucaususus', 'sinosteos', 'alaraptes', 'aracelosaudos']\n"
     ]
    }
   ],
   "source": [
    "names = generate_n_names(lstm_model, char_to_index=char_to_index, index_to_char=index_to_char, sequence_length=max_word_length, n=5, k=2)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k`= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conguionniodavithya', 'chesallientapatotho', 'priantaleor', 'stocusudhaleus', 'ausarodynnimiegitol']\n"
     ]
    }
   ],
   "source": [
    "names = generate_n_names(lstm_model, char_to_index=char_to_index, index_to_char=index_to_char, sequence_length=max_word_length, n=5, k=6)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) N-gram LSTM Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_name_generator-CGxwSDO1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
